%$\mathcal{N}(x\:|\: \mu, \sigma^{2}) = \frac{1}{\sigma \sqrt{2\pi}}\exp\bb{-\frac{1}{2\sigma^{2}}(x - \mu)^{2}}$,
%$\mathcal{N}_{n}(\bm{x}\:|\: \bm{\mu}, \bm{\Sigma}) =
%(2\pi)^{-\frac{n}{2}}\det(\bm{\Sigma})^{-\frac{1}{2}}\exp \big(-\frac{1}{2}(\bm{x} -
%\bm{\mu})^{\top}\bm{\Sigma}^{-1}(\bm{x} - \bm{\mu}) \big)$\\
%
%
%%\begin{itemize}
%%    \item $X \sim Binom(n, \theta): p(x|\theta) = \binom{n}{k}p^{k}(1-p)^{n-k}$, $\E {X}=np$, $Var(X)=np(1-p)$,
%%$\binom{n}{k} = \frac{n!}{k!(n-k)!}$
%%    \item $X \sim Lap(\mu, b)$, $\E {X} = \mu$, $\hat{\mu_{MLE}}$ = Median, $Var(X) = 2 b^2$
%%    \item $X \sim Unif(a, b)$, $x \in \left[ a,b \right]$,
%%$p(x|a,b) = \frac{1}{b-a} 1_{\left[ a,b \right]}(x)$,
%%$\E {X} = \frac{1}{2}(a+b)$, $\hat{\mu_{MLE}} = \frac{X_{(1)} + X_{(n)}}{2}$,
%%$Var(X) = \frac{1}{12}(b-a)^2$
%%    \item $X \sim Pois(\theta)$, $\theta > 0$, $x \in (0, \infty)$,
%%$p(x|\theta) = \exp (x \log(\theta)- \theta) \frac{1}{x!})$,
%%$\E {X} = \theta = Var(X)$
%%    \item $X \sim Exp(\theta)$, $p(x| \theta) = \exp ( \log(\theta) - \theta x)$
%%$\E {X} = \frac{1}{\theta}$,  $Var(X) =\frac{1}{\theta^2}$, $H(p) = 1- \log (\theta)$
%%
%%\end{itemize}
%
%\textbf{Random Variables $X,Y \in R$} \label{random vars}
%\begin{itemize}
%    \item $E[sgn(X)] = P(X>0)-P(X<0)$
%    \item $E[X] = \E[Y]{\E[X]{X|Y}}$
%    \item $Var(X) = \E {Var(X|Y)} + Var(\E {X|Y})$
%    \item $Cov(X,Y) = \E {XY} - \E {X}\E {Y} = \E{(X - \E {X})(Y - \E {Y})} = Cov(Y,X)$
%    \item $Cov(aX,Y) = aCov(X,Y)$, $Cov(X +c,Y)$
%    \item $Cov(X+Y, Z) = Cov(X,Z) + Cov(Y,Z)$
%    \item $Cov(X,Y)^{2} \leq Var(X)Var(Y)$
%    \item $Var(aX \pm bY) = a^{2}Var(X)+b^{2}Var(Y) \pm 2abCov(X,Y)$
%    \item $Var(XY) = \E {X^{2}Y^{2}} - \E {XY}^{2}$, if X,Y ind
%\end{itemize}
%\textbf{Random Vectors: $\bm{z},\ \bm{\mu}= \E {\bm{z}} \in R^{d}$}
%\begin{itemize}
%    \item $\bm{\Sigma} = COV(\bm{z}) = \E {\bm{z}\bm{z}^{\top}} - \E {\bm{z}} \E {\bm{z}}^{\top}$,
%    $(\bm{\Sigma})_{i,j} = Cov(z_{i}, z_{j})$
%    \item \boxed{1}: $\E {\bm{z}^{\top}\bm{A}\bm{z}} = tr(\bm{A}\bm{\Sigma}) + \bm{\mu}^{\top}\bm{A}\bm{\mu}$
%    \item $Var(\bm{a}^{\top}\bm{z}) = \bm{a}^{\top}\bm{\Sigma}\bm{a} \in R$, $Cov(\bm{A}\bm{z}) = \bm{A}\bm{\Sigma}\bm{A}^{\top}$
%\end{itemize}

\boxed{2}: $z \sim \mathcal{N}(0, I_p)$, R sym. and idempot. pxp, rank(R)=r. Then $z^{\top}Rz \sim \chi_{r}^{2}$.
\boxed{2.1}:$z \sim \mathcal{N}(\mu, \Sigma) \rightarrow (z-\mu)^{\top}\Sigma^{-1}(z-\mu) \sim \chi_{p}^{2}$, $z \in R^{p}$

\boxed{3}: F distribution: $X_{1} \sim \chi_{n}^{2}$, $X_{2} \sim \chi_{m}^{2}$, $X_1$ and $X_2$ ind.
$\rightarrow X = \frac{X_1/n}{X_2/m} \sim F_{n,m}$
\boxed{3.1}: $X=Y^2 \sim F_{1,m} \rightarrow Y = \sqrt {X} \sim t_m$
\boxed{3.1.2}: $X_1 \sim \mathcal{N}(0, I_n), X_2 \sim \chi_{n}^{2}$, $X_1$, $X_2$ ind.
$\rightarrow Y = \frac{X_1}{\sqrt {X_2/n}} \sim t_n$
Ex: $\frac{\hat{\sigma}^2}{\sigma^2}(n-p) \sim \chi^2_{n-p}$, $\frac{X-\mu}{\sigma} \sim \mathcal{N}(0,I)$
$\rightarrow \frac{X-\mu}{\hat{\sigma}} \sim t_{n-p}$

%
%
%\textbf{Multiple Testing}:\\
%\begin{tabular}{ |c|c|c|c| }
% \hline
%  & H_0 true & H_0 false & total \\
% \hline
% rej H_0 & V & S & R \\
% \hline
% not rej H_0 & U & w & m-R \\
% \hline
%  & m_0 & m-m_0 & m \\
% \hline
%\end{tabular}
%
%m: n of H, $m_0$: n of $H_0$  true, V: n Type 1 Error, w: n Type 2 Errors.
%
%\textbf{FWER} $= P(V \geq 1)$ = P of rej. $p^{(1)}$ (the smalles p-val)
%$= 1-P_{H_0}(p_{(i)} \geq \tau_{i}, \Forall {i})
%\overset{\tau_{i} = \alpha}{=} 1-(1-\alpha)^m
%\overset{\tau_{i} = \alpha/m, =\alpha/(m-i+1)}{\leq} \alpha$, decreases with pos. corr of p-vals.
%\textbf{FDR} $= \E {\frac{V}{R}}$.
%BH: under indep. or pos. dep. controls FDR at level $\alpha$. Find $P_(R) \leq \frac{\alpha R}{m}$
%\textrightarrow reject $H_0^{(1)}..H_0^{(R)}$
%\textbf{Non-parametric}: +: always correct, finite sample size (but then small power). -: workds only for global null test.
%
%%\textbf{GLM}:\\
%%\begin{tabular}{ |c|c|c|c|c|c|c|c|c| }
%% \hline
%%  & $\eta$ & $\E{y}=b'(\eta)$ & $b'(\eta)$ & $\Phi$ & $b''(\eta)$ & $s(\eta)$ & $ F(\eta)$ & $F(\beta)$\\
%% \hline
%% $\mathcal{N}(\mu, \sigma^2)$ & $\mu$ & $\mu = \eta$ & $\eta^2/2$ & $\sigma^2$ & 1
%% & $1/\sigma^2 (y-\mu)$ & $1/ \sigma' 2$ & $1/ \sigma^2 X^{\top}X$ \\
%% \hline
%% $\mathcal{N}(\mu, \sigma^2)$ & $\mu$ & $\mu = \eta$ & $\eta^2/2$ & $\sigma^2$ & 1
%% & $1/\sigma^2 (y-\mu)$ & $1/ \sigma' 2$ & $1/ \sigma^2 X^{\top}X$ \\
%% \hline
%% $\mathcal{N}(\mu, \sigma^2)$ & $\mu$ & $\mu = \eta$ & $\eta^2/2$ & $\sigma^2$ & 1
%% & $1/\sigma^2 (y-\mu)$ & $1/ \sigma' 2$ & $1/ \sigma^2 X^{\top}X$ \\
%% \hline
%%\end{tabular}
%
%
%
%\textbf{OLS}\\
%$LS(\beta')= \norm{y-X \beta'}^{2}_{2} = (\beta'-\hat{\beta}) X^{\top}X(\beta'-\hat{\beta})+\hat{\epsilon}^{\top}\hat{\epsilon}$
%$\overset{\beta'=\hat{\beta}}{=} \hat{\epsilon}^{\top}\hat{\epsilon}$ = SSE = RSS,
%assuming $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ corresponds to MLE.
%%$LS(\hat{\beta}) = (\beta-\hat{\beta}) X^{\top}X(\beta-\hat{\beta})+\hat{\epsilon}^{\top}\hat{\epsilon}$
%\textbf{Geometric Properties}
%$\hat{y}^{\top}\hat{\epsilon}=0, X^{\top}\epsilon = 0$,
%$\sum_{i=1}^{n} \hat{\epsilon}_{i} = 0,$ (if $\exists \beta_{0} $)
%$\bar{\hat{y}} = \bar{y} = \hat{\beta}_{0} + \hat{\beta}_{1}\bar{x}_{1} +...\hat{\beta}_{k}\bar{x}_{k}$
%
%\textbf{Hat Matrix}: $X(X^{\top}X)^{-1}X^{\top}$, $X \in R^{nxp}$.
%1) Idempotent: HH = H \textrightarrow rank(H)= tr(H) = p, 3) $\frac{1}{n} \leq H_{ii} \leq 1$,
%4) I-H is idempotent \textrightarrow rank(I-H) = tr(I-H) = tr(I) - tr(H) = n-p.
%$\hat{\epsilon} = (I-H)y = (I-H) (X \beta + \epsilon) = X \beta - HX \beta + (I-H)\epsilon=(I-H)\epsilon$
%
%\textbf{succ. ortho}: $\hat{B}_j = \frac{z^{j}^{\top}y}{z^{j}^{\top}z^{j}}$,
%$z^j$: $\hat{\epsilon}$ from regres. $x^j$ onto $x^{(-j)}$, $z^{j}^{\top}x^l=0, \Forall {l \neq j}$;
%if X orthog. \textrightarrow $x^j=z^j$ and $X^{\top}X$ diag.
%\textrightarrow $(X^{\top}X)^{-1}_{jj} = \frac{1}{x_{j}^{\top}x_{j}}$
%span($x^0..x^k$) = span($z^0..z^k$).
%$Var(\hat{\beta}_{j}) = \frac{\sigma^2}{z_{j}^{\top}z_j} = \sigma^2 (X^{\top}X)^{-1}_{jj}$,
%get's high when $x^{j}$ corr.
%$\hat{se}_j = \frac{\hat{\sigma}}{\norm{z^j}_2}$
%
%\textbf{$\bm{\hat{\sigma}}$:}
%$\hat{\sigma}^{2}_{ML} = \frac{1}{n} \hat{\epsilon}^{\top}\hat{\epsilon}$ is biased.
%$\sigma^{2} = \frac{1}{n-p}\hat{\epsilon}^{\top}\hat{\epsilon}$ unbiased.
%
%\textbf{RÂ²} = $\frac{\sum_{i=1}^{n} (\hat{y}_i- \bar{y})^2}
%{\sum_{i=1}^{n} (y_i- \bar{y})^2} = 1- \frac{SSE}{\sum_{i=1}^{n} (y_i- \bar{y})^2}$
%since $ \color{red} \frac{1}{n-1} \color{myblue}\sum (y_{i}-\bar{y})^2 =
% \color{red} \frac{1}{n-1} \color{myblue}\sum (\hat{y}_{i}-\bar{y})^2 +
%\color{red} \frac{1}{n-1} \color{myblue}\sum \hat{\epsilon}_i^2$
%$= \color{red} s_{y}^2 = s_{\hat{y}}^2 + s_{\hat{\epsilon}}^2 \color{myblue}$.
%$r =  \pm \sqrt {R^{2}}$