\section{Active Learning}
collect data that max. reduces uncertainty about unknown model.
\subsection{Mutual Information}
$I(X;Y)=H(X)-H(X|Y)=I(Y;X)$\\
$H(X|Y) = \E[y \sim p(y)]{H(X|Y=y)} = \E{\E{-\log p(X|Y)|Y=y}} =$
$\int_{\mathcal{Y}}\int_{\mathcal{X}} -\log p(x|y) p(x|y) p(y) dx dy=$
$\E[x,y \sim p(x,y)]{-\log \frac{p(x,y)}{p(y)}} =$
$\int_{\mathcal{Y}}\int_{\mathcal{X}} - \log p(x,y) p(x,y) dx dy$
$+ \underbrace{\int_{\mathcal{Y}}\int_{\mathcal{X}} \log p(y) p(x,y) dx dy}_{\int_{\mathcal{Y}} \log p(y) p(y)}
= H(X,Y) - H(Y)$
$ H(X|Y) = H(X) \leftrightarrow X \perp Y \leftrightarrow I(X;Y) = 0$\\
$H(X|Y)=0 \leftrightarrow \text{ X is completely determined by Y} \leftrightarrow  I(X;Y) = H(X) = H(Y)$\\
\textbf{EX1:} $X \sim \mathcal{N}(\mu, \Sigma),\ Y=X+\epsilon,
\epsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$,
$I(X;Y)=H(Y)-H(Y|X) = H(Y)-H(\epsilon) = \frac{1}{2} \log \abs{I+ \sigma_{n}^{-2}\Sigma}$

\textbf{Information gain:}
$F(S) := H(f)-H(f|y_{S}) = I(f;y_{S})$, $f: D \rightarrow R$, $S \subset D$\\
\textbf{\color{orange} EX2:}\color{black} $y_{S} = f(x_{S}) + \epsilon,\ \epsilon \sim \mathcal{N}(0, \sigma_{n}^{2}I)$, $f \sim GP(\mu, K)$
$p(y_{s}|f) = \mathcal{N}(f(x_{S}), \sigma_{n}^{2} I_{d})
\rightarrow H(y_{s}|f) = H(\epsilon)= \frac{1}{2}\log \bb{(2\pi e)^{d} \abs{\sigma_{n}^{2}I}}$,
$p(y_{s}) = \mathcal{N}(\mu(x_{S}), \sigma_{n}^{2} I_{d} + K_{S})
\rightarrow H(y_{S}) = H(f_{S} + \epsilon) =\frac{1}{2}\log \bb{(2\pi e)^{d} \abs{\sigma_{n}^{2}I + K_{S}}}$,
$I(f;y_{S}) = \frac{1}{2} \log \abs{I+ \sigma_{n}^{-2} K_{S}}$\\

Maximising F(S) is NP-hard \textrightarrow Greedy max mutual Inf:
$x_{t+1} = \argmax_{x}F(S_{t} \cup \{x\}) = \argmax_{x}F(S_{t} \cup \{x\}) -F(S_{t}) =
\argmax_{x}I(f; y_{x}|y_{S_{t}})
\overset{\color{orange} COV(\epsilon)=\sigma_{n}^{2}}{=}
\argmax_{x}\frac{1}{2} \log (1 + \frac{\color{blue}\sigma_{f}(x|S_{t})^{2}}{\color{green}{\sigma_{n}^{2}}})=
\color{myblue}\argmax_{x}\sigma_{t}(x)^{2}$ = uniform sampling = uncertainty sampl. (\color{blue} epistemic,
\color{green} aleatoric) \color{black}, $\sigma_{t}^2(x)=Var(f(x)|y_{1:t})$ (Posterior)
However in the heterosced. case fails to distinguish aleatoric and epistemic unc. and
most unc. outcomes are not necessarily most informative.

F(S) is monotone submodular \textrightarrow uncertainty sampling provides constant factor approx.
$\Forall {x \in D} \Forall {A \subseteq B \subseteq D} F(X|A) = F(X|B)$,
Marginal gain: $F(X|A)= I(f; y_{x} | y_{A})$

\subsection{Classification}
uncertainty sampl is max. entropy of prdicted label: $x_{t+1} \in \argmax_{x}H(Y|x,x_{1:t}, y_{1:t})
= - \sum_{y} \log p(y|x, x_{1:t}, y_{1:t})$
BALD max. mutual information: $x_{t+1} \in \argmax_{x}I(\theta; y_{x}|x_{1:t}, y_{1:t})
= - \sum_{y} \log p(y|x, x_{1:t}, y_{1:t}) = \underbrace{H(y|x, x_{1:t}, y_{1:t})}_{I} - $
$\underbrace{\E[\theta \sim p(\cdot|x_{1:t}, y_{1:t})]{H(y|x, \theta)}}_{II}$
$I$: entropy of avg. (approx by predictive distr. of approx. posterior)
$II$: average of entropy $\approx \frac{1}{m} \sum_{i=1}^{m}H(x, \theta^{(i)}),\ \theta^{(i)}\sim q(\cdot|y_{1:t})$

\section{Baysian Optimization}
How should we sequentially pick $x_{1},..x_{T}$ to find $\max_{x}f(x)$ with minimal samples.
$f \sim GP(0,k)$, Posterior: $\E{f(x)|y_{1:t}}=\mu_{t}(x), Var(f(x)|y_{1:t})=\sigma_{t}^2(x)$
\subsection{Aquisition functions}
\begin{itemize}
\end{itemize}




