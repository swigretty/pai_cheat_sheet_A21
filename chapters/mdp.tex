\section{Markov Decision Processes}
An MDP is specified by: states $X=\{1,..n\}$, actions $A=\{1,..m\}$, reward function $r(x,a, x')$,
transition prob: $P(x'|x, a)$

Given policy $\pi: X \rightarrow A$:
\begin{itemize}
    \item Expected cumulative rewards: $J(\pi) = \E{\sum_{t=0}^{\infty} \gamma^t r(X_t, \pi(X_t))} =
\sum_{x}P(X_{0}=x)V^{\pi}(x)$
    \item Value function: $V^{\pi}(x) & = J(\pi | X_0 = x) = \mathbb{E}[\sum_{t=0}^{\infty} \gamma^t r(X_t, \pi(X_t)) | X_0 = x] \\
         & = r(x, \pi(x)) + \gamma \sum_{x'} P(x' | x, \pi(x)) V^{\pi}(x')$\\
    \begin{itemize}
        \item compute $V^{\pi}$ exactly by solving linear system $=\mathcal{O}(n^3)$
        \item fixed point iteration: For $t=1:T V^{\pi}_{t} = r^{\pi} + \gamma T^{\pi} V_{t-1}^{\pi}$,
        will converge to $V^{*}$, since $B^{\pi}V$ is a contraction,
        i.e $\norm{V^{\pi_{t+1}}(x) - V^{*}}_{\infty} \leq \norm{BV^{\pi_{t}}(x) - V^{*}}_{\infty}$.
        and $V^{*}$ is a fixed point, i.e. $BV^{*} = V^{*}$.
        More efficint if $T^{\pi}$ is sparse.
    \end{itemize}
\end{itemize}

\textbf{Bellman:} Policy optimal $\leftrightarrow$ greedy wrt. to its induced value function:\\
$\pi^*(x)=\underset{a\in A}{argmax} [r(x,a)+\gamma \sum_{x'}P(x'|x,a)V^*(x')] =
\underset{a \in A}{argmax}Q^*(x,a)$\\
$V^*(x) = \max_{a}(r(x,a) + \gamma \sum_{x'}P(x'|x,a) V^*(x')) = \max_{a} \E[x']{r}$
$Q^*(x,a) = r(x,a) + \gamma \sum_{x'}P(x'|x,a) V^*(x') = \E[x']{r(x,a,x') + \gamma \max_{a}(Q^*(x',a))}$


\subsection{Policy iteration (Cost $O(n^3+nm\Delta)$)}
Start with an arbitrary (e.g. random) policy $\pi$.
Until converged, do:\\
- Compute value function $V^\pi (x)$\\
- Compute greedy policy $\pi_G$ w.r.t. $V^\pi$\\
- Set $\pi \leftarrow \pi_G$\\
Guaranteed to monotonically improve and to converge to $\pi^*$ in $O(n^2m/(1-\gamma))$, 
i.e. poly \# of iterations. However every iteration compute value function (e.g. fixed point iteration)

\subsection{Value iteration (Cost $O(nm\Delta)$)}
Initialize $V_0(x)=max_a r(x,a)$\\
For $t=1$ to $\infty$:\\
- For each $(x,a)$, let: \\
$Q_t(x,a)=r(x,a)+\gamma\sum_{x'}P(x'|x,a)V_{t-1}(x')$\\
- For each $x$, let $V_t(x)=\underset{a}{max}Q_t(x,a)$\\
- Break if $||V_t-V_{t-1}||_{\infty}=\underset{x}{max}|V_t(x)-V_{t-1}(x)|\leq\epsilon$\\
Then choose greedy policy w.r.t $V_t$.\\
Guaranteed to converge to $\epsilon$-optimal policy, in poly \# of iterations

\section{Reinforcement Learning}
$P(x'|x,a), r(x,a)$ not known. $\tau = \underbrace{x_{0}, a_{0}}_{\text{input}},
\underbrace{r_{0}, x_{1}}_{\text{label}}...)$. $D=\{\tau^{(1)}, ..., \tau^{(n )}\}$
Compared to supervised learning: data we get depends on actions \textrightarrow not i.i.d.

\textbf{on-poliy RL}: Agent has full control over actions, can choose how to trade exploration and exploitation.
Ex: TD-learning, $\epsilon$-greedy, SARSA\\
$Q(x,a) \leftarrow Q(x,a) + \alpha(r+\gammaQ(x', \pi(x'))- Q(x,a))$

\textbf{off-poliy RL}: no control over actions, get observational data. Ex: Q-learning, R_max
$Q(x,a) \leftarrow Q(x,a) + \alpha(r+\gamma max_{a'}Q(x', a')- Q(x,a))$

\subsection{model based}
estimate $P(x'|x,a), r(x,a)$ by calc avg. (exploration, choose best policy based on current estimate)
and simultaneous max. of J (exploitation)
$\hat{P}(X_{t+1}|X_{t},A) = \frac{Count(X_{t+1}, X_{t}, A)}{Count(X_t, A)}$,
$\hat{r}(x,a) = \frac{1}{N_{x,a}} \sum_{t:X_{t}=x, A_t=a} R(X_t, A_t)$\\

Memory: $\mathcal{O}(n^{2m})$, Comp time: solve multiple MDPs

\subsubsection{$\epsilon$ greedy}
With prob $\epsilon$ do random exploration, with prob $1-\epsilon$ choose greedy action according to current
$\hat{P}, \hat{r}$.
Will converge to $\pi^{*}$ with prob 1, if $\epsilon_{t}$ satisfies \color{magenta}RM:\color{black}
$\sum_{t=1}^{\infty}\epsilon_{t} = \infty$ and $\sum_{t=1}^{\infty}\epsilon_{t}^{2} < \infty$
e.g $\epsilon_{t}:\frac{1}{t^{k}},\ k \in (0,1],\max\cset{0.1,\frac{1}{t}}$ not valid: $\log(t)$


\subsubsection{R_{max}}
\begin{enumerate}
    \item init $r(x,a) = R_{max}, P(x^{\text{fairy}}|x,a)=1$ \textrightarrow optimal $\pi=$ random a
    \item Follow $\pi$, update $r(x,a), P(x'|x,a)$
    \begin{itemize}
        \item when observed enough: $\hat{r}$: Hoeffding bound with $C=R_{max}, N=n_{x,a}$\\
        recompute optimal $\pi$ based on current $\hat{r}, \hat{P}$. Repeat 2.
    \end{itemize}
\end{enumerate}
Number of steps required for $\epsilon$-optimal policy with prob $1-\delta$:
poly in $n,m,T,\frac{1}{\epsilon}, log(\frac{1}{\delta}, R_{max})$


\subsection{model free}
estimate $V^{\pi(x)}, Q$ directly

\subsection{TD-learing (on policy)}
Follow a given policy $\pi$ and obtain (x,a,r,x'):
$\hat{V^\pi} \leftarrow (1-\alpha_{t})\hat{V^\pi} + \alpha_t (r + \gamma \hat{V^\pi}(x'))$
current estimate added to reduce variance.
Converges to $V^\pi$ with prob 1 if $\alpha_{t}$ satisfies \color{magenta}RM:\color{black}.

\subsection{Q-learing (off-policy)}
$\hat{Q^*} \leftarrow (1-\alpha_{t})\hat{Q^*} + \alpha_t (r + \gamma \max_{a'}(\hat{Q^*}(x',a')))$
Converges to $Q^*$ with prob 1 if $\alpha_{t}$ satisfies \color{magenta}RM:\color{black} and
all (x,a) are chosen inf. often.
Memory: $\mathcal{O}(nm)$, Comp time: per transition $\max_{a'}(\hat{Q^*}(x',a')) = \mathcal{O}(m)$
However can be on-policy:
optimistic Q-learning (see R_max for $\epsilon$-optimal policy), $\epsilon$-greedy Q-learning.


\subsection{Policy Gradients}
\subsection{Actor Critic}








% TODO: look at POMDP
\subsection{POMDP = Belief-state MDP}
New states = beliefs over states for original POMDP\\
$B=\Delta({1,...,n})=\{ b:{1,...,n} \rightarrow [0,1],\sum_x b(x)=1 \}$\\
Actions: same as original MDP\\
\textbf{Transition model:}\\
- Stochastic observation:\\
$P(Y_t|b_t)=\sum_{x=1}^n P(Y_t|X_t=x)b_t(x)$\\
- State update (Bayesian filtering!), given $b_t, y_t, a_t$:
$b_{t+1}(x')=\frac{1}{Z}\sum_xb_t(x)P(y_t|x)P(X_{t+1}=x'|X_t=x,a_t)$\\
Reward function: $r(b_t, a_t)=\sum_x b_t(x)r(x,a_t)$

\subsection{Example of approx. solution to POMDPs: Policy gradients}
- Assume parameterized policy: $\pi(b)=\pi(b;\theta)$\\
- For each parameter $\theta$ the policy induces a Markov chain\\
- Can compute expected reward $J(\theta)$ by sampling.\\
- Find optimal parameters through search (gradient ascent):
$\theta^* = \underset{\theta}{arg max}\quad J(\theta)$


\begin{itemize}
    \item every finite MDP with bounded rewards and $gamma \in \left[ 0,1)$ has at least one optimal policy and
    unique value function
\end{itemize}